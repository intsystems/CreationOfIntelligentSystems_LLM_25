# Краткий обзор литературы

## 1. **Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning**
[arXiv:2012.13255](https://arxiv.org/abs/2012.13255)
- Предобученные языковые модели обладают **очень низкой внутренней (intrinsic) размерностью**.
- Даже при оптимизации всего **200 случайных параметров**, проецируемых обратно в полное пространство, можно достичь **90% производительности полной настройки** (на примере RoBERTa на MRPC).
- Предобучение неявно **минимизирует intrinsic dimension**, и более крупные модели после фиксированного числа шагов предобучения имеют **ещё более низкую** intrinsic dimension.
- Предложены **теоретические обобщённые оценки**, зависящие от intrinsic dimension, а не от общего числа параметров.

## 2. **Less is More: Local Intrinsic Dimensions of Contextual Language Models**
[arXiv:2506.01034](https://arxiv.org/pdf/2506.01034)
- Введён **унифицированный метод оценки локальной внутренней размерности** (с использованием TwoNN) в окрестностях отдельных токенов.
- Показано, что **локальная размерность** лучше отражает поведение модели, чем глобальная.
- Экспериментально установлена связь между **локальной размерностью и качеством обучения/валидации**.

## 3. **Semantic Structure in Large Language Model Embeddings**
[arXiv:2508.10003](https://arxiv.org/pdf/2508.10003)
- Семантически близкие слова имеют **высокую корреляцию** в эмбеддингах LLM.
- Антонимы также коррелируют, но их векторы направлены в **противоположные стороны** (высокая отрицательная косинусная схожесть).

## 4. **Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces**
[arXiv:2507.09709](https://arxiv.org/pdf/2507.09709)
- Семантика текстов по разным темам концентрируется в **линейных подпространствах малой размерности** (~10% от полной).
- Подпространства для разных тем **линейно разделимы**.
- Инструкции и alignment влияют на структуру этих подпространств.
- Знание подпространств позволяет строить **«латентные ограждения»** для защиты от адверсариальных атак.

## 5. **Analyzing Large Language Model Behavior via Embedding Analysis**
[OpenReview](https://openreview.net/pdf?id=mIWiqZOAtu)
- Токены разных тем формируют **стратифицированные многообразия** в эмбеддинговом пространстве.
- **Локальная размерность** этих многообразий коррелирует с качеством модели и признаками переобучения.
- Большой разрыв между локальной и полной размерностью — **индикатор переобучения**.

## 6. **Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the Limits of Embedding Space Capacity**
[arXiv:2502.13063 (2025), Kuratov et al.](https://arxiv.org/pdf/2502.13063)
- Исследуется **максимальная ёмкость эмбеддингового пространства**: авторы показывают, что можно сжать последовательность из **1568 токенов в один вектор** и затем точно восстановить исходную последовательность с помощью персонализированной оптимизации .
- Работа выявляет **большой разрыв между теоретической ёмкостью эмбеддингов и их практическим использованием** в существующих архитектурах .
- Предложенный подход заменяет стандартный энкодер процедурой оптимизации на уровне отдельного примера, демонстрируя потенциал сверхэффективного представления информации в LLM.

## 7. **Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts**
[arXiv:2306.04723 (2022), Tulchinskii et al.](https://arxiv.org/pdf/2306.04723)
- Предложен метод детекции генерируемого ИИ текста на основе **внутренней размерности многообразия эмбеддингов текста** .
- Используется оценка **PH-размерности** (на основе персистентной гомологии из топологического анализа данных), которая оказывается устойчивым инвариантом для человеческих текстов .
- Метод демонстрирует **робастность к перефразированию**, в отличие от многих классификаторов на основе поверхностных признаков .

---

### Общий вывод
Совокупность работ демонстрирует, что LLM эффективно работают не во всём параметрическом пространстве, а в **низкоразмерных, семантически осмысленных подпространствах**. Это объясняет их высокую обобщающую способность даже при малом объёме данных и открывает пути для интерпретируемости, компрессии, защиты и детекции машинно-сгенерированного контента.
